{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\freda\\miniconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\freda\\miniconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (2.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (0.23.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\freda\\miniconda3\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\freda\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\freda\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\freda\\miniconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\freda\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\freda\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Section imports\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import evaluate\n",
    "\n",
    "# Create variables and constants\n",
    "FOUNDATION_MODEL = \"gpt2\"\n",
    "tokenized_dataset = {}\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# Downloading the date set\n",
    "column_label = \"verse_text\"\n",
    "dataset_name='poem_sentiment'\n",
    "dataset = load_dataset(dataset_name, split='train').train_test_split(test_size=0.2, shuffle=True, seed=24)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined function to process tokenized inputs\n",
    "\n",
    "def tokenized_inputs(inputs, tokenizer):\n",
    "    prompt = inputs[column_label]\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_items = tokenizer(\n",
    "        prompt,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512\n",
    "    )\n",
    "    return tokenized_items\n",
    "\n",
    "\n",
    "\n",
    "# Defined function to evaluate accuracy of model\n",
    "# https://huggingface.co/spaces/evaluate-metric/precision\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"precision\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"micro\")\n",
    "\n",
    "\n",
    "# Created Generic function to return the Trainer object\n",
    "def get_trainer(trainer_model, dataset_name, metrics, num_epoch, output_dir, tokenizer):\n",
    "    trained_model = Trainer(\n",
    "        model=trainer_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=32,\n",
    "            per_device_eval_batch_size=96,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            num_train_epochs=num_epoch,\n",
    "            weight_decay=0.01,\n",
    "            load_best_model_at_end=True\n",
    "        ),\n",
    "        train_dataset=dataset_name[\"train\"],\n",
    "        eval_dataset=dataset_name[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    )\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "# Define Tokenizer function\n",
    "def get_tokenizer(modal_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modal_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_dataset Dataset({\n",
      "    features: ['id', 'verse_text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 713\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Configuring the tokenizer and adding padding tokens function\n",
    "tokenizer_model = get_tokenizer(FOUNDATION_MODEL)\n",
    "\n",
    "# Tokenize inputs\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenized_inputs(x, tokenizer_model), batched=True\n",
    "    )\n",
    "\n",
    "print(\"tokenized_dataset\", tokenized_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\freda\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating foundation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.239521026611328, 'eval_precision': 0.20670391061452514, 'eval_runtime': 3.8872, 'eval_samples_per_second': 46.049, 'eval_steps_per_second': 0.515}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Label objects to pass down to ouw model\n",
    "id2label = {0: \"negative\", 1: \"positive\", 2: \"no impact\", 3: \"mixed\"}\n",
    "label2id ={\"negative\": 0, \"positive\": 1, \"no impact\": 2, \"mixed\": 3}\n",
    "\n",
    "# Loading the foundation model with the correct number of labels and identifiers\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    FOUNDATION_MODEL,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer_model))\n",
    "model.to(device)\n",
    "\n",
    "# Created Trainer Object and loaded the original model\n",
    "gpt2_model_trainer = get_trainer(\n",
    "    trainer_model=model,\n",
    "    dataset_name=tokenized_dataset,\n",
    "    metrics=compute_metrics,\n",
    "    num_epoch=1,\n",
    "    output_dir=\"./\",\n",
    "    tokenizer=tokenizer_model\n",
    ")\n",
    "print(f'Evaluating foundation model...')\n",
    "original_model_eval = gpt2_model_trainer.evaluate()\n",
    "print(original_model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 150,528 || all params: 124,593,408 || trainable%: 0.1208\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRa settings\n",
    "targets = [\"c_proj\", \"c_fc\", \"c_attn\"]\n",
    "config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=targets,\n",
    "    fan_in_fan_out=True\n",
    ")\n",
    "# Create Peft Model\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze All parameters except those being fine tunes\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if any(target in name for target in targets):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\freda\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 23/23 [00:03<00:00,  5.13it/s]\n",
      "100%|██████████| 23/23 [00:05<00:00,  5.13it/s]c:\\Users\\freda\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.817417621612549, 'eval_precision': 0.25139664804469275, 'eval_runtime': 1.899, 'eval_samples_per_second': 94.26, 'eval_steps_per_second': 1.053, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:07<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7.4061, 'train_samples_per_second': 96.272, 'train_steps_per_second': 3.106, 'train_loss': 5.19460097603176, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.817417621612549, 'eval_precision': 0.25139664804469275, 'eval_runtime': 2.0208, 'eval_samples_per_second': 88.58, 'eval_steps_per_second': 0.99, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Train Object Using Lora Model from Peft\n",
    "lora_trainer = get_trainer(\n",
    "    trainer_model=lora_model, \n",
    "    dataset_name=tokenized_dataset, \n",
    "    metrics=compute_metrics, \n",
    "    num_epoch=1, \n",
    "    output_dir=\"./gpt-lora\",\n",
    "    tokenizer=tokenizer_model\n",
    ")\n",
    "lora_trainer.train()\n",
    "# Print lora_trainer evaluation\n",
    "eval_lora = lora_trainer.evaluate()\n",
    "print(eval_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\freda\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_model.save_pretrained(\"gpt-lora-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\freda\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.192325592041016, 'eval_precision': 0.20670391061452514, 'eval_runtime': 2.3547, 'eval_samples_per_second': 76.017, 'eval_steps_per_second': 0.849}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_lora_tokenizer = get_tokenizer(\"./gpt-lora/checkpoint-23\")\n",
    "# Loading saved PEFT model from directory\n",
    "lora_peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"./gpt-lora-trained\", num_labels=4)\n",
    "lora_peft_model.config.pad_token_id = lora_peft_model.config.eos_token_id\n",
    "lora_peft_model.resize_token_embeddings(len(tokenizer_model))\n",
    "lora_peft_model.to(device)\n",
    "\n",
    "# Load Trained Tokenizer\n",
    "lora_model_trainer = get_trainer(\n",
    "    trainer_model=lora_peft_model,\n",
    "    dataset_name=tokenized_dataset,\n",
    "    metrics=compute_metrics,\n",
    "    num_epoch=1,\n",
    "    output_dir=\"./\",\n",
    "    tokenizer=tokenizer_model\n",
    ")\n",
    "lora_model_eval = lora_model_trainer.evaluate()\n",
    "print(lora_model_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting Inference\n",
    "input_text = \"in prosperous days. like a dim, waning lamp\"\n",
    "model_inputs = new_lora_tokenizer(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = lora_peft_model(**model_inputs)\n",
    "    results = torch.argmax(outputs.logits, dim=-1)\n",
    "# Converting Prediction to Label\n",
    "predicted_label = id2label[results.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: in prosperous days. like a dim, waning lamp\n",
      " Predicted label: negative\n"
     ]
    }
   ],
   "source": [
    "# Input Text with Predicted outout\n",
    "print(f\"Input: {input_text}\\n Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab88aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model eval: {'eval_loss': 10.239521026611328, 'eval_precision': 0.20670391061452514, 'eval_runtime': 3.8872, 'eval_samples_per_second': 46.049, 'eval_steps_per_second': 0.515} \n",
      "Lora Model Eval: {'eval_loss': 10.192325592041016, 'eval_precision': 0.20670391061452514, 'eval_runtime': 2.3547, 'eval_samples_per_second': 76.017, 'eval_steps_per_second': 0.849}\n"
     ]
    }
   ],
   "source": [
    "# comparing Evaluation Results\n",
    "print(f'Original model eval: {original_model_eval} \\nLora Model Eval: {lora_model_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8c878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
